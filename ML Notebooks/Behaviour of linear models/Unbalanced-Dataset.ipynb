{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# What if Data is imabalanced\n",
    "\n",
    "<pre>\n",
    "1. As a part of this task you will observe how linear models work in case of data imbalanced\n",
    "2. observe how hyper plane is changs according to change in your learning rate.\n",
    "3. below we have created 4 random datasets which are linearly separable and having class imbalance\n",
    "4. in the first dataset the ratio between positive and negative is 100 : 2, in the 2nd data its 100:20, \n",
    "in the 3rd data its 100:40 and in 4th one its 100:80\n",
    "</pre>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = make_classification(n_samples=102,n_features=2,n_informative=2,n_redundant=0,n_classes=2,n_clusters_per_class=1,weights=[0.98],class_sep=1.5,random_state=57)\n",
    "df1 = pd.DataFrame(data1[0])\n",
    "df1['y'] = data1[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=df1[0],y=df1[1],hue=df1['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = make_classification(n_samples=120,n_features=2,n_informative=2,n_redundant=0,n_classes=2,n_clusters_per_class=1,weights=[0.8333],class_sep=1.55,random_state=10)\n",
    "df = pd.DataFrame(data[0])\n",
    "df['y'] = data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=df[0],y=df[1],hue=df['y'])"
   ]
  },
  {
   "source": [
    "`I the above case we can create an dataset of two classes but, fail to provide consistent format, as we can see the class 2 in the first data set is on the east side but in the second case it is on west side. So basically this method will not work.`"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(9)\n",
    "def sample_generator(one,m1,v1,two,m2,v2):\n",
    "    x1 = np.random.normal(m1,v1,(one,2))\n",
    "    y1 = np.zeros((one,1))\n",
    "    data1 = np.concatenate((x1,y1),axis=1)\n",
    "\n",
    "    x2 = np.random.normal(m2,v2,(two,2))\n",
    "    y2 = np.ones((two,1))\n",
    "    data2 = np.concatenate((x2,y2),axis=1)\n",
    "\n",
    "    data = np.concatenate((data1,data2),axis=0)\n",
    "    df = pd.DataFrame(data,columns=['f1','f2','y']).astype({'y': 'int32'})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (25,4)\n",
    "df1 = sample_generator(100,0,0.05,2,0.13,0.02)\n",
    "plt.subplot(141)\n",
    "sns.scatterplot(data=df1,x='f1',y='f2',hue='y')\n",
    "\n",
    "df2 = sample_generator(100,0,0.05,20,0.13,0.02)\n",
    "plt.subplot(142)\n",
    "sns.scatterplot(data=df2,x='f1',y='f2',hue='y')\n",
    "\n",
    "df3 = sample_generator(100,0,0.05,40,0.13,0.02)\n",
    "plt.subplot(143)\n",
    "sns.scatterplot(data=df3,x='f1',y='f2',hue='y')\n",
    "\n",
    "df4 = sample_generator(100,0,0.05,80,0.13,0.02)\n",
    "plt.subplot(144)\n",
    "sns.scatterplot(data=df4,x='f1',y='f2',hue='y')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "## Task 1: Applying SVM"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## RBF"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (25,14)\n",
    "f,axs = plt.subplots(3,4)\n",
    "i=0\n",
    "for c in [0.001, 1, 100]:\n",
    "    j=0\n",
    "    for df in [df1,df2,df3,df4]:\n",
    "        svc = SVC(C=c,kernel='rbf').fit(df[['f1','f2']],df['y'])\n",
    "\n",
    "        # create a mesh to plot in\n",
    "        x_min, x_max = df.f1.min() - 0.05, df.f1.max() + 0.05\n",
    "        y_min, y_max = df.f2.min() - 0.05, df.f2.max() + 0.05\n",
    "        xx2, yy2 = np.meshgrid(np.arange(x_min, x_max, .02),np.arange(y_min, y_max, .02))\n",
    "        Z = svc.predict(np.c_[xx2.ravel(), yy2.ravel()])\n",
    "        Z = Z.reshape(xx2.shape)\n",
    "        ax = axs[i][j]\n",
    "        ax.contourf(xx2, yy2, Z, cmap=plt.cm.coolwarm, alpha=0.2)\n",
    "        ax.scatter(df.f1, df.f2, c=df.y, cmap=plt.cm.coolwarm, s=25)\n",
    "        ax.axis([x_min, x_max,y_min, y_max])\n",
    "        j=j+1\n",
    "    i=i+1\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "## Linear SVM\n",
    "    with axes bounded"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (25,14)\n",
    "f,axs = plt.subplots(3,4)\n",
    "i=0\n",
    "for c in [0.001, 1, 100]:\n",
    "    j=0\n",
    "    for df in [df1,df2,df3,df4]:\n",
    "        svc = SVC(C=c,kernel='linear').fit(df[['f1','f2']],df['y'])\n",
    "\n",
    "        # create a mesh to plot in\n",
    "        x_min, x_max = df.f1.min() - 0.05, df.f1.max() + 0.05\n",
    "        y_min, y_max = df.f2.min() - 0.05, df.f2.max() + 0.05\n",
    "        xx2, yy2 = np.meshgrid(np.arange(x_min, x_max, .02),np.arange(y_min, y_max, .02))\n",
    "        Z = svc.predict(np.c_[xx2.ravel(), yy2.ravel()])\n",
    "        Z = Z.reshape(xx2.shape)\n",
    "\n",
    "        #Plotting Line\n",
    "        w = svc.coef_[0]\n",
    "        a = -w[0] / w[1]\n",
    "        xx = np.linspace(x_min, x_max)\n",
    "        yy = a * xx - (svc.intercept_[0]) /w[1]\n",
    "\n",
    "        #Plotting\n",
    "        ax = axs[i][j]\n",
    "        ax.contourf(xx2, yy2, Z, cmap=plt.cm.coolwarm, alpha=0.2)\n",
    "        ax.scatter(df.f1, df.f2, c=df.y, cmap=plt.cm.coolwarm, s=25)\n",
    "        ax.axis([x_min, x_max,y_min, y_max])\n",
    "        ax.plot(xx,yy,c='y')\n",
    "        j=j+1\n",
    "    i=i+1\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "## Linear SVM \n",
    "    without bounded axis"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (25,14)\n",
    "f,axs = plt.subplots(3,4)\n",
    "i=0\n",
    "for c in [0.001, 1, 100]:\n",
    "    j=0\n",
    "    for df in [df1,df2,df3,df4]:\n",
    "        svc = SVC(C=c,kernel='linear').fit(df[['f1','f2']],df['y'])\n",
    "\n",
    "        # create a mesh to plot in\n",
    "        x_min, x_max = df.f1.min() - 0.1, df.f1.max() + 0.15\n",
    "        y_min, y_max = df.f2.min() - 0.5, df.f2.max() + 0.1\n",
    "\n",
    "        #Plotting Line\n",
    "        w = svc.coef_[0]\n",
    "        a = -w[0] / w[1]\n",
    "        xx = np.linspace(x_min, x_max)\n",
    "        yy = a * xx - (svc.intercept_[0]) /w[1]\n",
    "\n",
    "        #prediction\n",
    "        xx2, yy2 = np.meshgrid(np.arange(x_min, x_max, .02),np.arange(y_min, y_max+(yy.max()*1.1), .02))\n",
    "        Z = svc.predict(np.c_[xx2.ravel(), yy2.ravel()])\n",
    "        Z = Z.reshape(xx2.shape)\n",
    "\n",
    "        #Plotting\n",
    "        ax = axs[i][j]\n",
    "        ax.contourf(xx2, yy2, Z, cmap=plt.cm.coolwarm, alpha=0.2)\n",
    "        ax.scatter(df.f1, df.f2, c=df.y, cmap=plt.cm.coolwarm, s=25)\n",
    "        # ax.axis([x_min, x_max,y_min, y_max])\n",
    "        ax.plot(xx,yy,c='y')\n",
    "        j=j+1\n",
    "    i=i+1\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "## Task 2: Applying LR\n",
    "    with bounded axis"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (25,14)\n",
    "f,axs = plt.subplots(3,4)\n",
    "i=0\n",
    "for c in [0.001, 1, 100]:\n",
    "    j=0\n",
    "    for df in [df1,df2,df3,df4]:\n",
    "        logiclf = LogisticRegression(C=c,fit_intercept=True,random_state=99,n_jobs=-1).fit(df[['f1','f2']],df['y'])\n",
    "\n",
    "        # create a mesh to plot in\n",
    "        x_min, x_max = df.f1.min() - 0.05, df.f1.max() + 0.05\n",
    "        y_min, y_max = df.f2.min() - 0.05, df.f2.max() + 0.05\n",
    "        xx2, yy2 = np.meshgrid(np.arange(x_min, x_max, .02),np.arange(y_min, y_max, .02))\n",
    "        Z = logiclf.predict(np.c_[xx2.ravel(), yy2.ravel()])\n",
    "        Z = Z.reshape(xx2.shape)\n",
    "\n",
    "        #Plotting Line\n",
    "        w = logiclf.coef_[0]\n",
    "        a = -w[0] / w[1]\n",
    "        xx = np.linspace(x_min, x_max)\n",
    "        yy = a * xx - (logiclf.intercept_[0]) /w[1]\n",
    "\n",
    "        #Plotting\n",
    "        ax = axs[i][j]\n",
    "        ax.contourf(xx2, yy2, Z, cmap=plt.cm.coolwarm, alpha=0.2)\n",
    "        ax.scatter(df.f1, df.f2, c=df.y, cmap=plt.cm.coolwarm, s=25)\n",
    "        ax.axis([x_min, x_max,y_min, y_max])\n",
    "        ax.plot(xx,yy,c='y')\n",
    "        j=j+1\n",
    "    i=i+1\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "## LR\n",
    "    without bounded axis\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (25,18)\n",
    "f,axs = plt.subplots(4,4)\n",
    "i=0\n",
    "for c in [0.001, 1,100, 100000000]: # the last causes the lambda to almost become zero hence no regularization\n",
    "    j=0\n",
    "    for df in [df1,df2,df3,df4]:\n",
    "        logiclf = LogisticRegression(C=c,fit_intercept=True,random_state=99,n_jobs=-1).fit(df[['f1','f2']],df['y'])\n",
    "\n",
    "        # create a mesh to plot in\n",
    "        x_min, x_max = df.f1.min() - 0.1, df.f1.max() + 0.15\n",
    "        y_min, y_max = df.f2.min() - 0.5, df.f2.max() + 0.1\n",
    "\n",
    "        #Plotting Line\n",
    "        w = logiclf.coef_[0]\n",
    "        a = -w[0] / w[1]\n",
    "        xx = np.linspace(x_min, x_max)\n",
    "        yy = a * xx - (logiclf.intercept_[0]) /w[1]\n",
    "\n",
    "        #prediction\n",
    "        xx2, yy2 = np.meshgrid(np.arange(x_min, x_max, .02),np.arange(y_min, y_max+(yy.max()*1.1), .02))\n",
    "        Z = logiclf.predict(np.c_[xx2.ravel(), yy2.ravel()])\n",
    "        Z = Z.reshape(xx2.shape)\n",
    "\n",
    "        #Plotting\n",
    "        ax = axs[i][j]\n",
    "        ax.contourf(xx2, yy2, Z, cmap=plt.cm.coolwarm, alpha=0.2)\n",
    "        ax.scatter(df.f1, df.f2, c=df.y, cmap=plt.cm.coolwarm, s=25)\n",
    "        # ax.axis([x_min, x_max,y_min, y_max])\n",
    "        ax.plot(xx,yy,c='y')\n",
    "        j=j+1\n",
    "    i=i+1\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "#### ` We can see that, as C is 1/Lambda for lower C values the seperation line is almost horizontal and way upp above in respect to the graph. This happens as both SVM and LR tries to minimise the loss by increasing the --- V.V. high bias. As the lambda decreases we get to see an more sensible line, as now the bias is decreasing. We need to find an prefect balance between the bias and variance by regulating the value of C.`\n",
    "\n",
    "`Now for an constant C, as the data becomes more balanced even with high regularization the coeff does not become zero easily, or leans to it's real(non regularization) position.\n",
    "This happens as the loss function has two parts one-the error from points and two-the regularization. So at \n",
    "1> High unbalanced/ balanced data & high regularization : The coeff (solpe of the line) tends to zero as the 2nd term of the loss function has more weigtage that the error function.\n",
    "2> Data becomes balanced : As we make the data balance, the significance of the error function starts to increase, as till now due to unbalance, the model could get away by completely declaring the region as the majority class and getting low error due to lack in population of minority class. But as the population of the minority class increases the error function starts getting higher values as the region is declared as major class and the minority class points also reside on the same side, this affects the loss function by making it hard to converge coeffs to zero by regularization, impact of error function increases. And hence it starts tilting/tending to the real line(non regularized) direction.\n",
    "3> Lower regularization: At lower lambdas the error function has considerable weitage and hence the line obtained is very similar to the non regularized line with some extra bias/lower variance.(IDEAL)`"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}