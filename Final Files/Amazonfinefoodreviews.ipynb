{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37764bitml1condab1e7a9cc0a4b4da2aa1261f0c90e368a",
   "display_name": "Python 3.7.7 64-bit ('ml1': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib notebook\n",
    "from tqdm import tqdm\n",
    "import sqlite3\n",
    "import gensim\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('../datafiles/amazon_reviews.sqlite')\n",
    "data = pd.read_sql_query(\"\"\" SELECT * FROM Reviews WHERE Score!=3\"\"\",conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleansing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scr(s):\n",
    "    if(s>3):\n",
    "        return 'positive'\n",
    "    else:\n",
    "        return 'negative'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0         5\n1         1\n2         4\n3         2\n4         5\n         ..\n525809    5\n525810    2\n525811    5\n525812    5\n525813    5\nName: Score, Length: 525814, dtype: int64"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "data['Score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Score'] = data['Score'].apply(scr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Index(['Id', 'ProductId', 'UserId', 'ProfileName', 'HelpfulnessNumerator',\n       'HelpfulnessDenominator', 'Score', 'Time', 'Summary', 'Text'],\n      dtype='object')"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(364171, 10)"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "cus_data = data.drop_duplicates(subset={'UserId','ProfileName', 'Time', 'Text'},keep='first')\n",
    "cus_data = cus_data[cus_data['HelpfulnessNumerator']<=cus_data['HelpfulnessDenominator']]\n",
    "cus_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "positive    307061\nnegative     57110\nName: Score, dtype: int64"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "cus_data['Score'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'Aboulutely love Popchips!I first tried these healthy chips at a marathon i did in California. I like this variety pack because i got to try alot of the flavors ive never had.'"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "cus_data.iloc[1500]['Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "            Id   ProductId          UserId                      ProfileName  \\\n0            1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n1            2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n2            3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n3            4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n4            5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n...        ...         ...             ...                              ...   \n525809  568450  B001EO7N10  A28KG5XORO54AY                 Lettie D. Carter   \n525810  568451  B003S1WTCU  A3I8AFVPEE8KI5                        R. Sawyer   \n525811  568452  B004I613EE  A121AA1GQV751Z                    pksd \"pk_007\"   \n525812  568453  B004I613EE   A3IBEVCTXKNOH          Kathy A. Welch \"katwel\"   \n525813  568454  B001LR2CU2  A3LGQPJCZVL9UC                         srfell17   \n\n        HelpfulnessNumerator  HelpfulnessDenominator     Score        Time  \\\n0                          1                       1  positive  1303862400   \n1                          0                       0  negative  1346976000   \n2                          1                       1  positive  1219017600   \n3                          3                       3  negative  1307923200   \n4                          0                       0  positive  1350777600   \n...                      ...                     ...       ...         ...   \n525809                     0                       0  positive  1299628800   \n525810                     0                       0  negative  1331251200   \n525811                     2                       2  positive  1329782400   \n525812                     1                       1  positive  1331596800   \n525813                     0                       0  positive  1338422400   \n\n                                   Summary  \\\n0                    Good Quality Dog Food   \n1                        Not as Advertised   \n2                    \"Delight\" says it all   \n3                           Cough Medicine   \n4                              Great taffy   \n...                                    ...   \n525809                 Will not do without   \n525810                        disappointed   \n525811            Perfect for our maltipoo   \n525812  Favorite Training and reward treat   \n525813                         Great Honey   \n\n                                                     Text  \n0       I have bought several of the Vitality canned d...  \n1       Product arrived labeled as Jumbo Salted Peanut...  \n2       This is a confection that has been around a fe...  \n3       If you are looking for the secret ingredient i...  \n4       Great taffy at a great price.  There was a wid...  \n...                                                   ...  \n525809  Great for sesame chicken..this is a good if no...  \n525810  I'm disappointed with the flavor. The chocolat...  \n525811  These stars are small, so you can give 10-15 o...  \n525812  These are the BEST treats for training and rew...  \n525813  I am very satisfied ,product is as advertised,...  \n\n[364171 rows x 10 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>ProductId</th>\n      <th>UserId</th>\n      <th>ProfileName</th>\n      <th>HelpfulnessNumerator</th>\n      <th>HelpfulnessDenominator</th>\n      <th>Score</th>\n      <th>Time</th>\n      <th>Summary</th>\n      <th>Text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>B001E4KFG0</td>\n      <td>A3SGXH7AUHU8GW</td>\n      <td>delmartian</td>\n      <td>1</td>\n      <td>1</td>\n      <td>positive</td>\n      <td>1303862400</td>\n      <td>Good Quality Dog Food</td>\n      <td>I have bought several of the Vitality canned d...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>B00813GRG4</td>\n      <td>A1D87F6ZCVE5NK</td>\n      <td>dll pa</td>\n      <td>0</td>\n      <td>0</td>\n      <td>negative</td>\n      <td>1346976000</td>\n      <td>Not as Advertised</td>\n      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>B000LQOCH0</td>\n      <td>ABXLMWJIXXAIN</td>\n      <td>Natalia Corres \"Natalia Corres\"</td>\n      <td>1</td>\n      <td>1</td>\n      <td>positive</td>\n      <td>1219017600</td>\n      <td>\"Delight\" says it all</td>\n      <td>This is a confection that has been around a fe...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>B000UA0QIQ</td>\n      <td>A395BORC6FGVXV</td>\n      <td>Karl</td>\n      <td>3</td>\n      <td>3</td>\n      <td>negative</td>\n      <td>1307923200</td>\n      <td>Cough Medicine</td>\n      <td>If you are looking for the secret ingredient i...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>B006K2ZZ7K</td>\n      <td>A1UQRSCLF8GW1T</td>\n      <td>Michael D. Bigham \"M. Wassir\"</td>\n      <td>0</td>\n      <td>0</td>\n      <td>positive</td>\n      <td>1350777600</td>\n      <td>Great taffy</td>\n      <td>Great taffy at a great price.  There was a wid...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>525809</th>\n      <td>568450</td>\n      <td>B001EO7N10</td>\n      <td>A28KG5XORO54AY</td>\n      <td>Lettie D. Carter</td>\n      <td>0</td>\n      <td>0</td>\n      <td>positive</td>\n      <td>1299628800</td>\n      <td>Will not do without</td>\n      <td>Great for sesame chicken..this is a good if no...</td>\n    </tr>\n    <tr>\n      <th>525810</th>\n      <td>568451</td>\n      <td>B003S1WTCU</td>\n      <td>A3I8AFVPEE8KI5</td>\n      <td>R. Sawyer</td>\n      <td>0</td>\n      <td>0</td>\n      <td>negative</td>\n      <td>1331251200</td>\n      <td>disappointed</td>\n      <td>I'm disappointed with the flavor. The chocolat...</td>\n    </tr>\n    <tr>\n      <th>525811</th>\n      <td>568452</td>\n      <td>B004I613EE</td>\n      <td>A121AA1GQV751Z</td>\n      <td>pksd \"pk_007\"</td>\n      <td>2</td>\n      <td>2</td>\n      <td>positive</td>\n      <td>1329782400</td>\n      <td>Perfect for our maltipoo</td>\n      <td>These stars are small, so you can give 10-15 o...</td>\n    </tr>\n    <tr>\n      <th>525812</th>\n      <td>568453</td>\n      <td>B004I613EE</td>\n      <td>A3IBEVCTXKNOH</td>\n      <td>Kathy A. Welch \"katwel\"</td>\n      <td>1</td>\n      <td>1</td>\n      <td>positive</td>\n      <td>1331596800</td>\n      <td>Favorite Training and reward treat</td>\n      <td>These are the BEST treats for training and rew...</td>\n    </tr>\n    <tr>\n      <th>525813</th>\n      <td>568454</td>\n      <td>B001LR2CU2</td>\n      <td>A3LGQPJCZVL9UC</td>\n      <td>srfell17</td>\n      <td>0</td>\n      <td>0</td>\n      <td>positive</td>\n      <td>1338422400</td>\n      <td>Great Honey</td>\n      <td>I am very satisfied ,product is as advertised,...</td>\n    </tr>\n  </tbody>\n</table>\n<p>364171 rows Ã— 10 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "cus_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">>>### Text Preprosessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Begin by removing the html tags\n",
    "- Remove any punctuations or limited set of special characters like , or . or # etc.\n",
    "- Check if the word is made up of english letters and is not alpha-numeric\n",
    "- Check to see if the length of the word is greater than 2 (as it was researched that there is no adjective in 2-letters)\n",
    "- Convert the word to lowercase\n",
    "- Remove Stopwords\n",
    "- Finally Snowball Stemming the word (it was obsereved to be better than Porter Stemming)\n",
    "\n",
    "After which we collect the words used to describe positive and negative reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">- ## cleaning html tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cln_html(sen):\n",
    "    clnd = re.sub(r'<.*?>',r' ',sen)\n",
    "    return clnd\n",
    "def cln_punc(sen):\n",
    "    clnd = re.sub(r'[?|!|\\'|\"|#]',r'',sen)\n",
    "    clnd = re.sub(r'[.|,|)|(|\\|/]',r' ',clnd)\n",
    "    return clnd    \n",
    "stop = set(stopwords.words('english'))\n",
    "sno = nltk.stem.SnowballStemmer('english') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do not run the code below as it takes a long time to run thorugh 36k sentences,\n",
    "the required data is stored in the sql file, whenever wanted use that sql file to obtain data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "str1=' '\n",
    "final_string=[]\n",
    "all_positive_words=[]\n",
    "all_negative_words=[]\n",
    "s=''\n",
    "for sent in cus_data['Text'].values:\n",
    "    filtered_sentence=[]\n",
    "    sent=cln_html(sent)\n",
    "    for w in sent.split():\n",
    "        for cleaned_words in cln_punc(w).split():\n",
    "            if((cleaned_words.isalpha()) & (len(cleaned_words)>2)):    \n",
    "                if(cleaned_words.lower() not in stop):\n",
    "                    s=(sno.stem(cleaned_words.lower())).encode('utf8')\n",
    "                    filtered_sentence.append(s)\n",
    "                    if (cus_data['Score'].values)[i] == 'positive': \n",
    "                        all_positive_words.append(s)\n",
    "                    if(cus_data['Score'].values)[i] == 'negative':\n",
    "                        all_negative_words.append(s)\n",
    "                else:\n",
    "                    continue\n",
    "            else:\n",
    "                continue \n",
    "    str1 = b\" \".join(filtered_sentence)\n",
    "    final_string.append(str1)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(len(final_string))\n",
    "final_string[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cus_data['CleanedText']=final_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Storing in SQL file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('../datafiles/cus_data.sqlite')\n",
    "c=conn.cursor()\n",
    "conn.text_factory = str\n",
    "cus_data.to_sql('Reviews', conn, schema=None, if_exists='replace', index=True, index_label=None, chunksize=None, dtype=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Checking the stored data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('../datafiles/cus_data.sqlite')\n",
    "temp_data = pd.read_sql_query(\"\"\" SELECT * FROM Reviews\"\"\",conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(temp_data.shape)\n",
    "temp_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Storing the cleaned sentences inn pickle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../datafiles/pickles/affr_clnd_sentences','wb') as affr_clnd_sentences:\n",
    "    pickle.dump(cus_data,affr_clnd_sentences)\n",
    "    pickle.dump(all_positive_words,affr_clnd_sentences)\n",
    "    pickle.dump(all_negative_words,affr_clnd_sentences)\n",
    "    affr_clnd_sentences.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Retriving the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../datafiles/pickles/affr_clnd_sentences','rb') as temp:\n",
    "    temp_cnldsentences = pickle.load(temp)\n",
    "    temp_pos =pickle.load(temp)\n",
    "temp_pos[5:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Checking the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempo = (temp_cnldsentences == cus_data)\n",
    "tempo.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ## Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer() #in scikit-learn\n",
    "bow = count_vect.fit_transform(cus_data['Text'].values) # .values has been depretiated use to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect_500 = CountVectorizer(max_features=500)\n",
    "bow_500 = count_vect_500.fit_transform(cus_data['Text'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(bow.dtype,'\\n',type(bow),'\\n',bow.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect_feat = count_vect.get_feature_names() # list of words in the BoW\n",
    "count_vect_feat.index('like')\n",
    "print(count_vect_feat[64055])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- BOW pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../datafiles/pickles/affr_bow','wb') as affr_bow:\n",
    "    pickle.dump(bow,affr_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- checking pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../datafiles/pickles/affr_bow','rb') as affr_bow:\n",
    "    temp = pickle.load(affr_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../datafiles/pickles/affr_bow_500','wb') as affr_bow_500:\n",
    "    pickle.dump(bow_500,affr_bow_500)\n",
    "with open('../datafiles/pickles/affr_bow_500','rb') as affr_bow_500:\n",
    "    temp = pickle.load(affr_bow_500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ## n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "freq_dist_positive=nltk.FreqDist(all_positive_words)\n",
    "freq_dist_negative=nltk.FreqDist(all_negative_words)\n",
    "print(\"Most Common Positive Words : \",freq_dist_positive.most_common(20))\n",
    "print(\"Most Common Negative Words : \",freq_dist_negative.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_grm = CountVectorizer(ngram_range=(1,2) )\n",
    "final_bigram_counts = n_grm.fit_transform(cus_data['Text'].to_numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> as you can see we put 1,2 in ngram_range hence we will get both 1 and 2 gram units. With reduce the abiguity as we see in above cells that the both the positive and negative words list contains words like 'taste' 'like' which gives wrong representation as the negative might contain 'dit not like' 'not tasty' which cannot be seen in Unigrame units, which is solved with Bigrame\n",
    "But it comes at the cost of dimentionality. i.e from 115,281 to 2,910,192"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_grm_500 = CountVectorizer(ngram_range=(1,2),max_features=500 )\n",
    "final_bigram_counts_500 = n_grm_500.fit_transform(cus_data['Text'].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_bigram_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pickilng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../datafiles/pickles/affr_bigram_bow','wb') as affr_bigram_bow:\n",
    "    pickle.dump(final_bigram_counts,affr_bigram_bow)\n",
    "with open('../datafiles/pickles/affr_bigram_bow','rb') as affr_bigram_bow:\n",
    "    temp = pickle.load(affr_bigram_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../datafiles/pickles/affr_bigram_bow_500','wb') as affr_bigram_bow_500:\n",
    "    pickle.dump(final_bigram_counts_500,affr_bigram_bow_500)\n",
    "with open('../datafiles/pickles/affr_bigram_bow_500','rb') as affr_bigram_bow_500:\n",
    "    temp = pickle.load(affr_bigram_bow_500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ## Tf-Idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TfidfVectorizer is Equivalent to CountVectorizer followed by TfidfTransformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf_idf = TfidfVectorizer().fit_transform(cus_data['Text'])\n",
    "tf_idf=TfidfVectorizer()\n",
    "tf_idf_vec =  tf_idf.fit_transform(cus_data['Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf2=TfidfVectorizer(ngram_range=(1,2))\n",
    "tf_idf2_vec =  tf_idf2.fit_transform(cus_data['Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(tf_idf2.get_feature_names()).iloc[100000:100010]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = tf_idf2.get_feature_names();\n",
    "tf_idf2.get_feature_names(); #output is hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf2.get_feature_names()[100000:100010]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# covnert a row in saprsematrix to a numpy array\n",
    "print(tf_idf2_vec[3,:].toarray()[0])\n",
    "print(\"------------------------------------------------------\")\n",
    "print(tf_idf2_vec[3,:]) \n",
    "print(\"------------------------------------------------------\")\n",
    "print(tf_idf2)\n",
    "print(\"------------------------------------------------------\")\n",
    "print(tf_idf2_vec)\n",
    "print(\"------------------------------------------------------\")\n",
    "type(tf_idf2_vec)\n",
    "print(\"------------------------------------------------------\")\n",
    "tf_idf2_vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source: https://buhrmann.github.io/tfidf-analysis.html\n",
    "def top_tfidf_feats(row, features, top_n=25):\n",
    "    ''' Get top n tfidf values in row and return them with their corresponding feature names.'''\n",
    "    topn_ids = np.argsort(row)[::-1][:top_n] # argsort returns the arguments of assendingly arranged data..[::1]->reverses the array.\n",
    "[:top_n]-> takes only top 25 \n",
    "    top_feats = [(features[i], row[i]) for i in topn_ids]\n",
    "    df = pd.DataFrame(top_feats)\n",
    "    df.columns = ['feature', 'tfidf']\n",
    "    return df\n",
    "\n",
    "top_tfidf = top_tfidf_feats(tf_idf2_vec[1,:].toarray()[0],features,25)\n",
    "top_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pickling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../datafiles/pickles/affr_unigram_tfidf','wb') as affr_unigram_tfidf:\n",
    "    pickle.dump(tf_idf,affr_unigram_tfidf)\n",
    "with open('../datafiles/pickles/affr_unigram_tfidf','rb') as affr_unigram_tfidf:\n",
    "    temp = pickle.load(affr_unigram_tfidf)\n",
    "temp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../datafiles/pickles/affr_ngram_tfidf','wb') as affr_ngram_tfidf:\n",
    "    pickle.dump(tf_idf2_vec,affr_ngram_tfidf)\n",
    "with open('../datafiles/pickles/affr_ngram_tfidf','rb') as affr_ngram_tfidf:\n",
    "    temp = pickle.load(affr_ngram_tfidf)\n",
    "temp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf2_500=TfidfVectorizer(ngram_range=(1,2),max_features=500)\n",
    "tf_idf2_vec_500 =  tf_idf2_500.fit_transform(cus_data['Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../datafiles/pickles/affr_ngram_tfidf_500','wb') as affr_ngram_tfidf_500:\n",
    "    pickle.dump(tf_idf2_vec_500,affr_ngram_tfidf_500)\n",
    "with open('../datafiles/pickles/affr_ngram_tfidf_500','rb') as affr_ngram_tfidf_500:\n",
    "    temp = pickle.load(affr_ngram_tfidf_500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- # word2Vec "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "import pickle\n",
    "\n",
    "- ### in this project we are using a pretrained model by google\n",
    "\n",
    "- ### its 3.3G file, once you load this into your memory \n",
    "\n",
    "- ### it occupies ~9Gb, so please do this step only if you have >12G of ram\n",
    "\n",
    "- ### we will provide a pickle file wich contains a dict , \n",
    "\n",
    "- ### and it contains all our courpus words as keys and  model[word] as values\n",
    "\n",
    "- ### To use this code-snippet, download \"GoogleNews-vectors-negative300.bin\" \n",
    "\n",
    "- ### from https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit\n",
    "\n",
    "- ### it's 1.9GB in size.\n",
    "\n",
    "model = KeyedVectors.load_word2vec_format('../datafiles/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model.wv['computer']\n",
    "\n",
    "model.wv.similarity('woman', 'man')\n",
    "\n",
    "model.wv.most_similar('woman')\n",
    "\n",
    "model.wv.most_similar('tasti')  # \"tasti\" is the stemmed word for tasty, tastful\n",
    "\n",
    "model.wv.most_similar('tasty') \n",
    "\n",
    "model.wv.similarity('tasty', 'tast')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if you do NOT have RAM >= 12GB, use the code below.\n",
    "# But we dont have this pickle, when you will get it, then we will be able to run th model.\n",
    "\n",
    "# with open('word2vec_model', 'rb') as handle:\n",
    "#     model = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### These are word list of sentences without stop words or stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train your own Word2Vec model using your own text corpus\n",
    "\n",
    "list_of_sent=[]\n",
    "for sent in cus_data['Text'].values:\n",
    "    filtered_sentence=[]\n",
    "    sent=cln_html(sent)\n",
    "    for w in sent.split():\n",
    "        for cleaned_words in cln_punc(w).split():\n",
    "            if(cleaned_words.isalpha()):    \n",
    "                filtered_sentence.append(cleaned_words.lower())\n",
    "            else:\n",
    "                continue\n",
    "    list_of_sent.append(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list_of_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../datafiles/pickles/unstoppedunstemmed_word_list_sent','wb') as unstoppedunstemmed_word_list_sent:\n",
    "    pickle.dump(list_of_sent,unstoppedunstemmed_word_list_sent)\n",
    "with open('../datafiles/pickles/unstoppedunstemmed_word_list_sent','rb') as unstoppedunstemmed_word_list_sent:\n",
    "    temp = pickle.load(unstoppedunstemmed_word_list_sent)\n",
    "len(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(cus_data['Text'].values[0])\n",
    "print(\"*****************************************************************\")\n",
    "print(list_of_sent[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training our W2v model with list_of_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model=gensim.models.Word2Vec(list_of_sent,min_count=5,size=50, workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Saving the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import get_tmpfile\n",
    "path = get_tmpfile(\"../datafiles/pickles/w2v_model.model\")\n",
    "w2v_model.save(\"../datafiles/pickles/w2v_model.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp_w2v_model = gensim.models.Word2Vec.load(\"../datafiles/pickles/w2v_model.model\")\n",
    "w2v_model = gensim.models.Word2Vec.load(\"../datafiles/pickles/w2v_model.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Checking/Comparing the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "words = list(temp_w2v_model.wv.vocab)\n",
    "print(len(words))\n",
    "words = list(w2v_model.wv.vocab)\n",
    "print(len(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OBSERVATION : We have 33783 unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.wv.most_similar('tasty')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_w2v_model.wv.most_similar('tasty')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The code below will not work as the amount of data is tooooo large for pickle to handle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('../datafiles/pickles/clnd_sentences_w2v','wb') as clnd_sentences_for_w2v:\n",
    "#     pickle.dump(list_of_sent[:200000],clnd_sentences_for_w2v)\n",
    "# with open('../datafiles/pickles/clnd_sentences_w2v','rb') as clnd_sentences_for_w2v:\n",
    "#     temp = pickle.load(clnd_sentences_for_w2v)\n",
    "# len(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- So insted of pickle we use Joblib from sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "joblib.dump(list_of_sent,\"../datafiles/pickles/clnd_sentences_for_w2v.joblib\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " temp = joblib.load('../datafiles/pickles/clnd_sentences_for_w2v.joblib') \n",
    " len(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "//////////////////////////////  some more word surfing //////////////////////////////////"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.wv.most_similar('like')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(w2v_model.wv['like'].shape)\n",
    "w2v_model.wv['like']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.wv['sahil']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_sent[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ## Avg W2V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# average Word2Vec\n",
    "# compute average word2vec for each review.\n",
    "i=0\n",
    "sent_vectors = []; # the avg-w2v for each sentence/review is stored in this list\n",
    "for sent in list_of_sent: # for each review/sentence\n",
    "    sent_vec = np.zeros(50) # as word vectors are of zero length\n",
    "    cnt_words =0; # num of words with a valid vector in the sentence/review\n",
    "    for word in sent: # for each word in a review/sentence\n",
    "        try:\n",
    "            vec = w2v_model.wv[word]\n",
    "            sent_vec += vec\n",
    "            cnt_words += 1\n",
    "            i=i+1\n",
    "        except:\n",
    "            pass\n",
    "    sent_vec /= cnt_words\n",
    "    sent_vectors.append(sent_vec)\n",
    "print(len(sent_vectors))\n",
    "print(len(sent_vectors[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_vectors[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Storing in pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../datafiles/pickles/avg_w2v','wb') as avg_w2v:\n",
    "    pickle.dump(sent_vectors,avg_w2v)\n",
    "with open('../datafiles/pickles/avg_w2v','rb') as avg_w2v:\n",
    "    temp = pickle.load(avg_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ## TfIdf wiighted W2V "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if u dont want to execute the list_of_sent generating function\n",
    "with open('../datafiles/pickles/unstoppedunstemmed_word_list_sent','rb') as unstoppedunstemmed_word_list_sent:\n",
    "    list_of_sent = pickle.load(unstoppedunstemmed_word_list_sent)\n",
    "# and dont want to train the w2v model again\n",
    "with open('../datafiles/pickles/avg_w2v','rb') as avg_w2v:\n",
    "    w2v_model = gensim.models.Word2Vec.load(\"../datafiles/pickles/w2v_model.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TF-IDF weighted Word2Vec\n",
    "tfidf_feat = tf_idf.get_feature_names() \n",
    "# final_tf_idf is the sparse matrix with row= sentence, col=word and cell_val = tfidf\n",
    "\n",
    "tfidf_sent_vectors = []; # the tfidf-w2v for each sentence/review is stored in this list\n",
    "row=0;\n",
    "for sent in tqdm(list_of_sent[0:1000]): # for each review/sentence\n",
    "    sent_vec = np.zeros(50) # as word vectors are of zero length\n",
    "    weight_sum =0; # num of words with a valid vector in the sentence/review\n",
    "    for word in sent: # for each word in a review/sentence\n",
    "        try:\n",
    "            vec = w2v_model.wv[word]\n",
    "            # obtain the tf_idfidf of a word in a sentence/review\n",
    "            tfidf = tf_idf_vec[row, tfidf_feat.index(word)]\n",
    "            sent_vec += (vec * tfidf)\n",
    "            weight_sum += tfidf\n",
    "        except:\n",
    "            pass\n",
    "    sent_vec /= weight_sum\n",
    "    tfidf_sent_vectors.append(sent_vec)\n",
    "    row += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../datafiles/pickles/tdifd_weighted_w2v','wb') as tdifd_weighted_w2v:\n",
    "    pickle.dump(tfidf_sent_vectors,tdifd_weighted_w2v)\n",
    "with open('../datafiles/pickles/tdifd_weighted_w2v','rb') as tdifd_weighted_w2v:\n",
    "    temp = pickle.load(tdifd_weighted_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(temp),temp[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}