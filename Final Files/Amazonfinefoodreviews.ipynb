{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38264bitplaygroundconda181cc4e8a1f74f20aba28f8bf4ca7131",
   "display_name": "Python 3.8.2 64-bit ('playground': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import sqlite3\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('../datafiles/amazon_reviews.sqlite')\n",
    "data = pd.read_sql_query(\"\"\" SELECT * FROM Reviews WHERE Score!=3\"\"\",conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleansing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scr(s):\n",
    "    if(s>3):\n",
    "        return 'positive'\n",
    "    else:\n",
    "        return 'negative'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0         5\n1         1\n2         4\n3         2\n4         5\n         ..\n525809    5\n525810    2\n525811    5\n525812    5\n525813    5\nName: Score, Length: 525814, dtype: int64"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "data['Score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Score'] = data['Score'].apply(scr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Index(['Id', 'ProductId', 'UserId', 'ProfileName', 'HelpfulnessNumerator',\n       'HelpfulnessDenominator', 'Score', 'Time', 'Summary', 'Text'],\n      dtype='object')"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(364171, 10)"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "cus_data = data.drop_duplicates(subset={'UserId','ProfileName', 'Time', 'Text'},keep='first')\n",
    "cus_data = cus_data[cus_data['HelpfulnessNumerator']<=cus_data['HelpfulnessDenominator']]\n",
    "cus_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "positive    307061\nnegative     57110\nName: Score, dtype: int64"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "cus_data['Score'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'Aboulutely love Popchips!I first tried these healthy chips at a marathon i did in California. I like this variety pack because i got to try alot of the flavors ive never had.'"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "cus_data.iloc[1500]['Text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">>>### Text Preprosessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Begin by removing the html tags\n",
    "- Remove any punctuations or limited set of special characters like , or . or # etc.\n",
    "- Check if the word is made up of english letters and is not alpha-numeric\n",
    "- Check to see if the length of the word is greater than 2 (as it was researched that there is no adjective in 2-letters)\n",
    "- Convert the word to lowercase\n",
    "- Remove Stopwords\n",
    "- Finally Snowball Stemming the word (it was obsereved to be better than Porter Stemming)\n",
    "\n",
    "After which we collect the words used to describe positive and negative reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">- ## cleaning html tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cln_html(sen):\n",
    "    clnd = re.sub(r'<.*?>',r' ',sen)\n",
    "    return clnd\n",
    "def cln_punc(sen):\n",
    "    clnd = re.sub(r'[?|!|\\'|\"|#]',r'',sen)\n",
    "    clnd = re.sub(r'[.|,|)|(|\\|/]',r' ',clnd)\n",
    "    return clnd    \n",
    "stop = set(stopwords.words('english'))\n",
    "sno = nltk.stem.SnowballStemmer('english') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "str1=' '\n",
    "final_string=[]\n",
    "all_positive_words=[]\n",
    "all_negative_words=[]\n",
    "s=''\n",
    "for sent in cus_data['Text'].values:\n",
    "    filtered_sentence=[]\n",
    "    sent=cln_html(sent)\n",
    "    for w in sent.split():\n",
    "        for cleaned_words in cln_punc(w).split():\n",
    "            if((cleaned_words.isalpha()) & (len(cleaned_words)>2)):    \n",
    "                if(cleaned_words.lower() not in stop):\n",
    "                    s=(sno.stem(cleaned_words.lower())).encode('utf8')\n",
    "                    filtered_sentence.append(s)\n",
    "                    if (cus_data['Score'].values)[i] == 'positive': \n",
    "                        all_positive_words.append(s)\n",
    "                    if(cus_data['Score'].values)[i] == 'negative':\n",
    "                        all_negative_words.append(s)\n",
    "                else:\n",
    "                    continue\n",
    "            else:\n",
    "                continue \n",
    "    str1 = b\" \".join(filtered_sentence)\n",
    "    final_string.append(str1)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cus_data['CleanedText']=final_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'cus_data' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-077d19cc9f44>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcursor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext_factory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mcus_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_sql\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Reviews'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mif_exists\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'replace'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex_label\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'cus_data' is not defined"
     ]
    }
   ],
   "source": [
    "conn = sqlite3.connect('../datafiles/cus_data.sqlite')\n",
    "c=conn.cursor()\n",
    "conn.text_factory = str\n",
    "cus_data.to_sql('Reviews', conn, schema=None, if_exists='replace', index=True, index_label=None, chunksize=None, dtype=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('../datafiles/cus_data.sqlite')\n",
    "data = pd.read_sql_query(\"\"\" SELECT * FROM Reviews\"\"\",conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "         index      Id   ProductId          UserId  \\\n0            0       1  B001E4KFG0  A3SGXH7AUHU8GW   \n1            1       2  B00813GRG4  A1D87F6ZCVE5NK   \n2            2       3  B000LQOCH0   ABXLMWJIXXAIN   \n3            3       4  B000UA0QIQ  A395BORC6FGVXV   \n4            4       5  B006K2ZZ7K  A1UQRSCLF8GW1T   \n...        ...     ...         ...             ...   \n364166  525809  568450  B001EO7N10  A28KG5XORO54AY   \n364167  525810  568451  B003S1WTCU  A3I8AFVPEE8KI5   \n364168  525811  568452  B004I613EE  A121AA1GQV751Z   \n364169  525812  568453  B004I613EE   A3IBEVCTXKNOH   \n364170  525813  568454  B001LR2CU2  A3LGQPJCZVL9UC   \n\n                            ProfileName  HelpfulnessNumerator  \\\n0                            delmartian                     1   \n1                                dll pa                     0   \n2       Natalia Corres \"Natalia Corres\"                     1   \n3                                  Karl                     3   \n4         Michael D. Bigham \"M. Wassir\"                     0   \n...                                 ...                   ...   \n364166                 Lettie D. Carter                     0   \n364167                        R. Sawyer                     0   \n364168                    pksd \"pk_007\"                     2   \n364169          Kathy A. Welch \"katwel\"                     1   \n364170                         srfell17                     0   \n\n        HelpfulnessDenominator     Score        Time  \\\n0                            1  positive  1303862400   \n1                            0  negative  1346976000   \n2                            1  positive  1219017600   \n3                            3  negative  1307923200   \n4                            0  positive  1350777600   \n...                        ...       ...         ...   \n364166                       0  positive  1299628800   \n364167                       0  negative  1331251200   \n364168                       2  positive  1329782400   \n364169                       1  positive  1331596800   \n364170                       0  positive  1338422400   \n\n                                   Summary  \\\n0                    Good Quality Dog Food   \n1                        Not as Advertised   \n2                    \"Delight\" says it all   \n3                           Cough Medicine   \n4                              Great taffy   \n...                                    ...   \n364166                 Will not do without   \n364167                        disappointed   \n364168            Perfect for our maltipoo   \n364169  Favorite Training and reward treat   \n364170                         Great Honey   \n\n                                                     Text  \\\n0       I have bought several of the Vitality canned d...   \n1       Product arrived labeled as Jumbo Salted Peanut...   \n2       This is a confection that has been around a fe...   \n3       If you are looking for the secret ingredient i...   \n4       Great taffy at a great price.  There was a wid...   \n...                                                   ...   \n364166  Great for sesame chicken..this is a good if no...   \n364167  I'm disappointed with the flavor. The chocolat...   \n364168  These stars are small, so you can give 10-15 o...   \n364169  These are the BEST treats for training and rew...   \n364170  I am very satisfied ,product is as advertised,...   \n\n                                              CleanedText  \n0       b'bought sever vital can dog food product foun...  \n1       b'product arriv label jumbo salt peanut peanut...  \n2       b'confect around centuri light pillowi citrus ...  \n3       b'look secret ingredi robitussin believ found ...  \n4       b'great taffi great price wide assort yummi ta...  \n...                                                   ...  \n364166  b'great sesam chicken good better restur eaten...  \n364167  b'disappoint flavor chocol note especi weak mi...  \n364168  b'star small give one train session tri train ...  \n364169  b'best treat train reward dog good groom lower...  \n364170  b'satisfi product advertis use cereal raw vine...  \n\n[364171 rows x 12 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>index</th>\n      <th>Id</th>\n      <th>ProductId</th>\n      <th>UserId</th>\n      <th>ProfileName</th>\n      <th>HelpfulnessNumerator</th>\n      <th>HelpfulnessDenominator</th>\n      <th>Score</th>\n      <th>Time</th>\n      <th>Summary</th>\n      <th>Text</th>\n      <th>CleanedText</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>1</td>\n      <td>B001E4KFG0</td>\n      <td>A3SGXH7AUHU8GW</td>\n      <td>delmartian</td>\n      <td>1</td>\n      <td>1</td>\n      <td>positive</td>\n      <td>1303862400</td>\n      <td>Good Quality Dog Food</td>\n      <td>I have bought several of the Vitality canned d...</td>\n      <td>b'bought sever vital can dog food product foun...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>2</td>\n      <td>B00813GRG4</td>\n      <td>A1D87F6ZCVE5NK</td>\n      <td>dll pa</td>\n      <td>0</td>\n      <td>0</td>\n      <td>negative</td>\n      <td>1346976000</td>\n      <td>Not as Advertised</td>\n      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n      <td>b'product arriv label jumbo salt peanut peanut...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>3</td>\n      <td>B000LQOCH0</td>\n      <td>ABXLMWJIXXAIN</td>\n      <td>Natalia Corres \"Natalia Corres\"</td>\n      <td>1</td>\n      <td>1</td>\n      <td>positive</td>\n      <td>1219017600</td>\n      <td>\"Delight\" says it all</td>\n      <td>This is a confection that has been around a fe...</td>\n      <td>b'confect around centuri light pillowi citrus ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>4</td>\n      <td>B000UA0QIQ</td>\n      <td>A395BORC6FGVXV</td>\n      <td>Karl</td>\n      <td>3</td>\n      <td>3</td>\n      <td>negative</td>\n      <td>1307923200</td>\n      <td>Cough Medicine</td>\n      <td>If you are looking for the secret ingredient i...</td>\n      <td>b'look secret ingredi robitussin believ found ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>5</td>\n      <td>B006K2ZZ7K</td>\n      <td>A1UQRSCLF8GW1T</td>\n      <td>Michael D. Bigham \"M. Wassir\"</td>\n      <td>0</td>\n      <td>0</td>\n      <td>positive</td>\n      <td>1350777600</td>\n      <td>Great taffy</td>\n      <td>Great taffy at a great price.  There was a wid...</td>\n      <td>b'great taffi great price wide assort yummi ta...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>364166</th>\n      <td>525809</td>\n      <td>568450</td>\n      <td>B001EO7N10</td>\n      <td>A28KG5XORO54AY</td>\n      <td>Lettie D. Carter</td>\n      <td>0</td>\n      <td>0</td>\n      <td>positive</td>\n      <td>1299628800</td>\n      <td>Will not do without</td>\n      <td>Great for sesame chicken..this is a good if no...</td>\n      <td>b'great sesam chicken good better restur eaten...</td>\n    </tr>\n    <tr>\n      <th>364167</th>\n      <td>525810</td>\n      <td>568451</td>\n      <td>B003S1WTCU</td>\n      <td>A3I8AFVPEE8KI5</td>\n      <td>R. Sawyer</td>\n      <td>0</td>\n      <td>0</td>\n      <td>negative</td>\n      <td>1331251200</td>\n      <td>disappointed</td>\n      <td>I'm disappointed with the flavor. The chocolat...</td>\n      <td>b'disappoint flavor chocol note especi weak mi...</td>\n    </tr>\n    <tr>\n      <th>364168</th>\n      <td>525811</td>\n      <td>568452</td>\n      <td>B004I613EE</td>\n      <td>A121AA1GQV751Z</td>\n      <td>pksd \"pk_007\"</td>\n      <td>2</td>\n      <td>2</td>\n      <td>positive</td>\n      <td>1329782400</td>\n      <td>Perfect for our maltipoo</td>\n      <td>These stars are small, so you can give 10-15 o...</td>\n      <td>b'star small give one train session tri train ...</td>\n    </tr>\n    <tr>\n      <th>364169</th>\n      <td>525812</td>\n      <td>568453</td>\n      <td>B004I613EE</td>\n      <td>A3IBEVCTXKNOH</td>\n      <td>Kathy A. Welch \"katwel\"</td>\n      <td>1</td>\n      <td>1</td>\n      <td>positive</td>\n      <td>1331596800</td>\n      <td>Favorite Training and reward treat</td>\n      <td>These are the BEST treats for training and rew...</td>\n      <td>b'best treat train reward dog good groom lower...</td>\n    </tr>\n    <tr>\n      <th>364170</th>\n      <td>525813</td>\n      <td>568454</td>\n      <td>B001LR2CU2</td>\n      <td>A3LGQPJCZVL9UC</td>\n      <td>srfell17</td>\n      <td>0</td>\n      <td>0</td>\n      <td>positive</td>\n      <td>1338422400</td>\n      <td>Great Honey</td>\n      <td>I am very satisfied ,product is as advertised,...</td>\n      <td>b'satisfi product advertis use cereal raw vine...</td>\n    </tr>\n  </tbody>\n</table>\n<p>364171 rows × 12 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ## Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'CountVectorizer' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-f078beb6a5c0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcount_vect\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#in scikit-learn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mbow\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcount_vect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# .values has been depretiated use to_numpy()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'CountVectorizer' is not defined"
     ]
    }
   ],
   "source": [
    "count_vect = CountVectorizer() #in scikit-learn\n",
    "bow = count_vect.fit_transform(data['Text'].values) # .values has been depretiated use to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "int64 \n <class 'scipy.sparse.csr.csr_matrix'> \n (364171, 115281)\n"
    }
   ],
   "source": [
    "print(bow.dtype,'\\n',type(bow),'\\n',bow.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ## n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Most Common Positive Words :  [(b'like', 139426), (b'tast', 129045), (b'good', 112766), (b'flavor', 109628), (b'love', 107357), (b'use', 103886), (b'great', 103871), (b'one', 96723), (b'product', 91033), (b'tri', 86790), (b'tea', 83893), (b'coffe', 78813), (b'make', 75107), (b'get', 72124), (b'food', 64803), (b'would', 55566), (b'time', 55264), (b'buy', 54198), (b'realli', 52714), (b'eat', 52004)]\nMost Common Negative Words :  [(b'tast', 34587), (b'like', 32333), (b'product', 28218), (b'one', 20572), (b'flavor', 19571), (b'would', 17974), (b'tri', 17754), (b'use', 15304), (b'good', 15041), (b'coffe', 14717), (b'get', 13787), (b'buy', 13752), (b'order', 12871), (b'food', 12753), (b'dont', 11877), (b'tea', 11660), (b'even', 11088), (b'box', 10843), (b'amazon', 10073), (b'make', 9840)]\n"
    }
   ],
   "source": [
    "freq_dist_positive=nltk.FreqDist(all_positive_words)\n",
    "freq_dist_negative=nltk.FreqDist(all_negative_words)\n",
    "print(\"Most Common Positive Words : \",freq_dist_positive.most_common(20))\n",
    "print(\"Most Common Negative Words : \",freq_dist_negative.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_grm = CountVectorizer(ngram_range=(1,2) )\n",
    "final_bigram_counts = n_grm.fit_transform(data['Text'].to_numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> as you can see we put 1,2 in ngram_range hence we will get both 1 and 2 gram units. With reduce the abiguity as we see in above cells that the both the positive and negative words list contains words like 'taste' 'like' which gives wrong representation as the negative might contain 'dit not like' 'not tasty' which cannot be seen in Unigrame units, which is solved with Bigrame\n",
    "But it comes at the cost of dimentionality. i.e from 115,281 to 2,910,192"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<364171x2910192 sparse matrix of type '<class 'numpy.int64'>'\n\twith 45049660 stored elements in Compressed Sparse Row format>"
     },
     "metadata": {},
     "execution_count": 61
    }
   ],
   "source": [
    "final_bigram_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ## Tf-Idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TfidfVectorizer is Equivalent to CountVectorizer followed by TfidfTransformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf = TfidfVectorizer().fit_transform(data['Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf2=TfidfVectorizer(  ngram_range=(1,2))\n",
    "tf_idf2_vec =  tf_idf2.fit_transform(data['Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0       00\n1    00 00\n2    00 07\n3    00 09\n4    00 10\ndtype: object"
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "pd.Series(tf_idf2.get_feature_names()).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = tf_idf2.get_feature_names();\n",
    "tf_idf2.get_feature_names(); #output is hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['ales until',\n 'ales ve',\n 'ales would',\n 'ales you',\n 'alessandra',\n 'alessandra ambrosia',\n 'alessi',\n 'alessi added',\n 'alessi also',\n 'alessi and']"
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "tf_idf2.get_feature_names()[100000:100010]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[0. 0. 0. ... 0. 0. 0.]\n\n  (0, 2746741)\t0.20864406459243112\n  (0, 1328386)\t0.07471461675685441\n  (0, 966736)\t0.08577072448646748\n  (0, 2521977)\t0.07525969862030189\n  (0, 2305119)\t0.16720704162458458\n  (0, 519921)\t0.1788777240190179\n  (0, 2312964)\t0.21231435167256238\n  (0, 1505491)\t0.1452554084765105\n  (0, 151277)\t0.1203337236728053\n  (0, 1092035)\t0.09241910984219587\n  (0, 2774516)\t0.10850684818597585\n  (0, 2822510)\t0.1119547534739983\n  (0, 1805781)\t0.2144686151689472\n  (0, 900675)\t0.23240808025553095\n  (0, 305841)\t0.23240808025553095\n  (0, 2141485)\t0.1401240393842999\n  (0, 2533496)\t0.16061453189794056\n  (0, 74015)\t0.11043777388007137\n  (0, 1267174)\t0.11761910694232527\n  (0, 2578626)\t0.10058011930205456\n  (0, 1101627)\t0.1133438791277944\n  (0, 1336429)\t0.12012212218074879\n  (0, 1009523)\t0.09654312355740105\n  (0, 313959)\t0.185494900101343\n  (0, 2136821)\t0.23982407761531543\n  :\t:\n  (0, 313760)\t0.09142321641094835\n  (0, 2136820)\t0.2070526178091627\n  (0, 1293846)\t0.09689792714336101\n  (0, 2200057)\t0.12849635939315268\n  (0, 991870)\t0.029811639088999108\n  (0, 1481157)\t0.07571217758995002\n  (0, 2618821)\t0.06281597364241154\n  (0, 2891956)\t0.07976741847149828\n  (0, 1257191)\t0.055879715561427956\n  (0, 187768)\t0.03906264003391605\n  (0, 2891706)\t0.038389806156715535\n  (0, 2745180)\t0.04528288905517418\n  (0, 1266669)\t0.0607318365133144\n  (0, 2771454)\t0.03941502726868648\n  (0, 1254867)\t0.046686375345189116\n  (0, 1171732)\t0.09728991285689109\n  (0, 2574871)\t0.02634827712676156\n  (0, 1317342)\t0.027293727498259708\n  (0, 1332766)\t0.025774428722616288\n  (0, 1091864)\t0.04249144490706597\n  (0, 2606992)\t0.02520549945683493\n  (0, 1008621)\t0.06305823780394984\n  (0, 139736)\t0.022285336233218427\n  (0, 2512439)\t0.06526311875533479\n  (0, 1169043)\t0.03569158894475856\nTfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n                dtype=<class 'numpy.float64'>, encoding='utf-8',\n                input='content', lowercase=True, max_df=1.0, max_features=None,\n                min_df=1, ngram_range=(1, 2), norm='l2', preprocessor=None,\n                smooth_idf=True, stop_words=None, strip_accents=None,\n                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                tokenizer=None, use_idf=True, vocabulary=None)\n  (0, 2499050)\t0.10234947416198675\n  (0, 326318)\t0.07073576296397971\n  (0, 1996693)\t0.1548057387254361\n  (0, 2580733)\t0.052850895677459174\n  (0, 184694)\t0.19517611308138588\n  (0, 2231832)\t0.19050236287778582\n  (0, 156921)\t0.08730313800947702\n  (0, 953161)\t0.1567192694581234\n  (0, 1321424)\t0.15597503702636847\n  (0, 1403055)\t0.20643714422127335\n  (0, 1644522)\t0.16562163972530902\n  (0, 325578)\t0.1461055842637026\n  (0, 2287621)\t0.15503296248227272\n  (0, 1340246)\t0.10197841843601003\n  (0, 150034)\t0.05470456451980995\n  (0, 1553482)\t0.11280848972355047\n  (0, 1986700)\t0.17924133173789836\n  (0, 2499662)\t0.16748037703748744\n  (0, 2378859)\t0.19813815171634205\n  (0, 1449430)\t0.1748234330232663\n  (0, 1612759)\t0.09492618969819948\n  (0, 1482153)\t0.15458164918946013\n  (0, 1998081)\t0.15597503702636847\n  (0, 2531908)\t0.06872465186404055\n  (0, 2033488)\t0.1327994732207163\n  :\t:\n  (364170, 2752165)\t0.19586813931184374\n  (364170, 2752125)\t0.15301145350473727\n  (364170, 498972)\t0.11884342399051624\n  (364170, 141178)\t0.1500799339605786\n  (364170, 1338388)\t0.1136164738686185\n  (364170, 2715258)\t0.11509387371964606\n  (364170, 2445518)\t0.18385573744480815\n  (364170, 2053504)\t0.1390977332265429\n  (364170, 205224)\t0.17638520207652483\n  (364170, 1318340)\t0.15799196334324075\n  (364170, 80244)\t0.15736642558175848\n  (364170, 1997902)\t0.11714493002073012\n  (364170, 2747355)\t0.175063934138578\n  (364170, 126199)\t0.1414338575827877\n  (364170, 2171676)\t0.14421007507526543\n  (364170, 123667)\t0.08047566972955071\n  (364170, 2713716)\t0.07687112625740271\n  (364170, 1766118)\t0.05280116353346219\n  (364170, 2745180)\t0.06159338487706414\n  (364170, 2843637)\t0.04808472180385199\n  (364170, 204974)\t0.11296476303780184\n  (364170, 1317342)\t0.03712468655614475\n  (364170, 1332766)\t0.03505814980939615\n  (364170, 1989288)\t0.06304392110533977\n  (364170, 139736)\t0.03031231708857949\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(364171, 2910192)"
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "source": [
    "# covnert a row in saprsematrix to a numpy array\n",
    "print(tf_idf2_vec[3,:].toarray()[0])\n",
    "print()\n",
    "print(tf_idf2_vec[3,:]) \n",
    "print(tf_idf2)\n",
    "print(tf_idf2_vec)\n",
    "type(tf_idf2_vec)\n",
    "tf_idf2_vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "            feature     tfidf\n0          as jumbo  0.390489\n1             jumbo  0.260971\n2      unsalted not  0.201475\n3      jumbo salted  0.201475\n4   vendor intended  0.201475\n5    sized unsalted  0.201475\n6   arrived labeled  0.187395\n7           peanuts  0.186777\n8    actually small  0.184594\n9          error or  0.176745\n10    represent the  0.162063\n11     to represent  0.161483\n12   salted peanuts  0.157496\n13      small sized  0.154333\n14     peanuts were  0.150833\n15      peanuts the  0.149735\n16         an error  0.146627\n17        represent  0.140615\n18    were actually  0.133485\n19      the peanuts  0.127286\n20       labeled as  0.127042\n21      intended to  0.126160\n22         unsalted  0.122116\n23       the vendor  0.117553\n24            error  0.116271",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>feature</th>\n      <th>tfidf</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>as jumbo</td>\n      <td>0.390489</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>jumbo</td>\n      <td>0.260971</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>unsalted not</td>\n      <td>0.201475</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>jumbo salted</td>\n      <td>0.201475</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>vendor intended</td>\n      <td>0.201475</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>sized unsalted</td>\n      <td>0.201475</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>arrived labeled</td>\n      <td>0.187395</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>peanuts</td>\n      <td>0.186777</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>actually small</td>\n      <td>0.184594</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>error or</td>\n      <td>0.176745</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>represent the</td>\n      <td>0.162063</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>to represent</td>\n      <td>0.161483</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>salted peanuts</td>\n      <td>0.157496</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>small sized</td>\n      <td>0.154333</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>peanuts were</td>\n      <td>0.150833</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>peanuts the</td>\n      <td>0.149735</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>an error</td>\n      <td>0.146627</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>represent</td>\n      <td>0.140615</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>were actually</td>\n      <td>0.133485</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>the peanuts</td>\n      <td>0.127286</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>labeled as</td>\n      <td>0.127042</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>intended to</td>\n      <td>0.126160</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>unsalted</td>\n      <td>0.122116</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>the vendor</td>\n      <td>0.117553</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>error</td>\n      <td>0.116271</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 46
    }
   ],
   "source": [
    "# source: https://buhrmann.github.io/tfidf-analysis.html\n",
    "def top_tfidf_feats(row, features, top_n=25):\n",
    "    ''' Get top n tfidf values in row and return them with their corresponding feature names.'''\n",
    "    topn_ids = np.argsort(row)[::-1][:top_n] # argsort returns the arguments of assendingly arranged data..[::1]->reverses the array..[:top_n]-> takes only top 25 \n",
    "    top_feats = [(features[i], row[i]) for i in topn_ids]\n",
    "    df = pd.DataFrame(top_feats)\n",
    "    df.columns = ['feature', 'tfidf']\n",
    "    return df\n",
    "\n",
    "top_tfidf = top_tfidf_feats(tf_idf2_vec[1,:].toarray()[0],features,25)\n",
    "top_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- # word2Vec "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "import pickle\n",
    "\n",
    "- ### in this project we are using a pretrained model by google\n",
    "\n",
    "- ### its 3.3G file, once you load this into your memory \n",
    "\n",
    "- ### it occupies ~9Gb, so please do this step only if you have >12G of ram\n",
    "\n",
    "- ### we will provide a pickle file wich contains a dict , \n",
    "\n",
    "- ### and it contains all our courpus words as keys and  model[word] as values\n",
    "\n",
    "- ### To use this code-snippet, download \"GoogleNews-vectors-negative300.bin\" \n",
    "\n",
    "- ### from https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit\n",
    "\n",
    "- ### it's 1.9GB in size.\n",
    "\n",
    "model = KeyedVectors.load_word2vec_format('../datafiles/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model.wv['computer']\n",
    "\n",
    "model.wv.similarity('woman', 'man')\n",
    "\n",
    "model.wv.most_similar('woman')\n",
    "\n",
    "model.wv.most_similar('tasti')  # \"tasti\" is the stemmed word for tasty, tastful\n",
    "\n",
    "model.wv.most_similar('tasty') \n",
    "\n",
    "model.wv.similarity('tasty', 'tast')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train your own Word2Vec model using your own text corpus\n",
    "import gensim\n",
    "i=0\n",
    "list_of_sent=[]\n",
    "for sent in final['Text'].values:\n",
    "    filtered_sentence=[]\n",
    "    sent=cleanhtml(sent)\n",
    "    for w in sent.split():\n",
    "        for cleaned_words in cleanpunc(w).split():\n",
    "            if(cleaned_words.isalpha()):    \n",
    "                filtered_sentence.append(cleaned_words.lower())\n",
    "            else:\n",
    "                continue \n",
    "    list_of_sent.append(filtered_sentence)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final['Text'].values[0])\n",
    "print(\"*****************************************************************\")\n",
    "print(list_of_sent[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model=gensim.models.Word2Vec(list_of_sent,min_count=5,size=50, workers=4)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = list(w2v_model.wv.vocab)\n",
    "print(len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.wv.most_similar('tasty')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.wv.most_similar('like')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect_feat = count_vect.get_feature_names() # list of words in the BoW\n",
    "count_vect_feat.index('like')\n",
    "print(count_vect_feat[64055])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ## Avg W2V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average Word2Vec\n",
    "# compute average word2vec for each review.\n",
    "sent_vectors = []; # the avg-w2v for each sentence/review is stored in this list\n",
    "for sent in list_of_sent: # for each review/sentence\n",
    "    sent_vec = np.zeros(50) # as word vectors are of zero length\n",
    "    cnt_words =0; # num of words with a valid vector in the sentence/review\n",
    "    for word in sent: # for each word in a review/sentence\n",
    "        try:\n",
    "            vec = w2v_model.wv[word]\n",
    "            sent_vec += vec\n",
    "            cnt_words += 1\n",
    "        except:\n",
    "            pass\n",
    "    sent_vec /= cnt_words\n",
    "    sent_vectors.append(sent_vec)\n",
    "print(len(sent_vectors))\n",
    "print(len(sent_vectors[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ## TfIdf-W2V "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF weighted Word2Vec\n",
    "tfidf_feat = tf_idf_vect.get_feature_names() # tfidf words/col-names\n",
    "# final_tf_idf is the sparse matrix with row= sentence, col=word and cell_val = tfidf\n",
    "\n",
    "tfidf_sent_vectors = []; # the tfidf-w2v for each sentence/review is stored in this list\n",
    "row=0;\n",
    "for sent in list_of_sent: # for each review/sentence\n",
    "    sent_vec = np.zeros(50) # as word vectors are of zero length\n",
    "    weight_sum =0; # num of words with a valid vector in the sentence/review\n",
    "    for word in sent: # for each word in a review/sentence\n",
    "        try:\n",
    "            vec = w2v_model.wv[word]\n",
    "            # obtain the tf_idfidf of a word in a sentence/review\n",
    "            tfidf = final_tf_idf[row, tfidf_feat.index(word)]\n",
    "            sent_vec += (vec * tf_idf)\n",
    "            weight_sum += tf_idf\n",
    "        except:\n",
    "            pass\n",
    "    sent_vec /= weight_sum\n",
    "    tfidf_sent_vectors.append(sent_vec)\n",
    "    row += 1"
   ]
  }
 ]
}